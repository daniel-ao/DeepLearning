{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1WbnlsUEWGvHjPlsxOdp2syBobV2667DF","timestamp":1731670204744}],"authorship_tag":"ABX9TyMchJ4BmVoJvFpVEpMiOrm5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# TP régression logistique et réseau de neurones\n","\n","Le but de ce TP est de coder à la main un modèle de régression logistique et un réseau de neurones classique, en implémentant soit-même le calcul du gradient.\n","\n","Nous allons tester nos implémentations sur le dataset [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist).\n","\n","Rien ne vous est demandé à part l'implémentation des modèles. Je vous conseille tout de même de bien regarder la façon dont est structurée le reste du code car vous aurez à le faire vous-même pour les futurs TPs.\n","\n","**Conseil général:**\n","\n","- **Implémentez toutes les opérations à l'aide d'opérations matricielles.**\n","- **Pensez à bien vérifier les shapes des matrices que vous manipulez. L'analyse des shapes peut suffire pour déduire la bonne implémentation 90% du temps.**\n","- **Ce TP peut se faire sans utiliser une seule boucle for.**"],"metadata":{"id":"7kAgOaXFu5JV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5O4whiqM-e4_"},"outputs":[],"source":["!pip install numpy matplotlib openml scikit-learn tqdm > /dev/null"]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import openml\n","from sklearn.model_selection import train_test_split\n","\n","dataset = openml.datasets.get_dataset(\"Fashion-MNIST\")\n","df, _, _, _ = dataset.get_data()\n","\n","df_train, df_test = train_test_split(df, test_size=0.20, random_state=0)\n","\n","X_train, y_train = df_train.to_numpy()[:, :-1], df_train[\"class\"].values\n","X_test, y_test = df_test.to_numpy()[:, :-1], df_test[\"class\"].values\n","\n","X_train, y_train = X_train.astype(int), y_train.astype(int)\n","X_test, y_test = X_test.astype(int), y_test.astype(int)"],"metadata":{"id":"03gmc5JH_V3j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataset\n","\n","L'objet `Dataset` permet de charger des batchs de données aléatoires afin d'être utilisés lors de la SGD."],"metadata":{"id":"SFHZBD8kvgvP"}},{"cell_type":"code","source":["from typing import Iterator\n","\n","\n","class Dataset:\n","    def __init__(self, X: np.ndarray, y: np.ndarray, n_classes: int = 10, seed: int = 0):\n","        self.X = X\n","        self.y = y\n","        self.n_classes = n_classes\n","        self.rng = np.random.default_rng(seed)\n","\n","    def __getitem__(self, i: int) -> tuple[np.ndarray, np.ndarray]:\n","        \"\"\"Fetch the given sample.\n","\n","        ---\n","        Args:\n","            i: Sample id.\n","\n","        ---\n","        Returns:\n","            x: The flattened image.\n","                Shape of [n_pixels,].\n","            y: The class of the image as a one-hot vector.\n","                Shape of [n_classes,].\n","        \"\"\"\n","        x = self.X[i] / 255\n","        y = np.zeros(self.n_classes, float)\n","        y[self.y[i]] = 1\n","        return x, y\n","\n","    def __len__(self) -> int:\n","        \"\"\"Number of samples in the dataset.\"\"\"\n","        return len(self.X)\n","\n","    def iterate(self, batch_size: int) -> Iterator[tuple[np.ndarray, np.ndarray]]:\n","        \"\"\"Returns an iterator going over all samples of the dataset in a random\n","        order.\n","        \"\"\"\n","        indices = self.rng.permutation(len(self))\n","        for i in range(0, len(indices), batch_size):\n","            batch = range(i, min(i + batch_size, len(indices)))\n","            batch = [indices[j] for j in batch]\n","            batch = [self[j] for j in batch]\n","            batch = Dataset.collate_fn(batch)\n","            yield batch\n","\n","    @staticmethod\n","    def collate_fn(batch: list[tuple[np.ndarray, np.ndarray]]) -> tuple[np.ndarray, np.ndarray]:\n","        \"\"\"Stack a list of samples into a batch of numpy arrays.\n","\n","        ---\n","        Args:\n","            batch: The list of (x, y) tuples.\n","\n","        ---\n","        Returns:\n","            xs: The stacked images.\n","                Shape of [batch_size, n_pixels].\n","            ys: The stacked classes.\n","                Shape of [batch_size, n_classes].\n","        \"\"\"\n","        xs = [b[0] for b in batch]\n","        ys = [b[1] for b in batch]\n","\n","        xs = np.stack(xs)\n","        ys = np.stack(ys)\n","        return xs, ys\n","\n","    @staticmethod\n","    def show_images(X: np.ndarray, y: np.ndarray, n_cols: int = 8, imsize: int = 28) -> plt.Figure:\n","        n_images = len(X)\n","        X = (X * 255).astype(int)\n","        X = X.reshape(n_images, imsize, imsize)\n","        y = np.argmax(y, axis=1)\n","\n","        id_to_name = {\n","            0: \"T-shirt/top\",\n","            1: \"Trouser\",\n","            2: \"Pullover\",\n","            3: \"Dress\",\n","            4: \"Coat\",\n","            5: \"Sandal\",\n","            6: \"Shirt\",\n","            7: \"Sneaker\",\n","            8: \"Bag\",\n","            9: \"Ankle boot\",\n","        }\n","\n","        fig, axes = plt.subplots(n_images // n_cols, n_cols)\n","        for ax, xi, yi in zip(axes.flatten(), X, y):\n","            ax.imshow(xi, cmap='gray')\n","            ax.set_title(f\"{id_to_name[yi]}\")\n","            ax.set_xticks([])\n","            ax.set_yticks([])\n","\n","        fig.tight_layout()\n","        return fig\n","\n","train_dataset = Dataset(X_train, y_train, seed=0)\n","test_dataset = Dataset(X_test, y_test, seed=1)\n","\n","print(f\"Training size: {len(train_dataset):,}\")\n","print(f\"Testing size: {len(test_dataset):,}\")"],"metadata":{"id":"iszoERS8BBL7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X, y_true = next(iter(train_dataset.iterate(16)))\n","fig = Dataset.show_images(X, y_true, n_cols=4)\n","fig.suptitle(\"Training images\")\n","fig.tight_layout()\n","plt.show(fig)"],"metadata":{"id":"tvWV0g5uWppU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Trainer\n","\n","L'objet `Trainer` sert de coordinateur général pour entraîner le modèle sur plusieurs epochs et périodiquement l'évaluer sur le test set.\n","\n","Vous devez implémenter les fonctions `accuracy` et `loss`."],"metadata":{"id":"P9_Q4ujYvXD9"}},{"cell_type":"code","source":["from tqdm.notebook import tqdm\n","\n","\n","class Trainer:\n","    def __init__(\n","        self,\n","        model,\n","        train_dataset: Dataset,\n","        test_dataset: Dataset,\n","        learning_rate: float,\n","        batch_size: int,\n","        n_epochs: int,\n","    ):\n","        self.model = model\n","        self.train_dataset = train_dataset\n","        self.test_dataset = test_dataset\n","        self.learning_rate = learning_rate\n","        self.batch_size = batch_size\n","        self.n_epochs = n_epochs\n","\n","    def train(self):\n","        \"\"\"Train the model for one epoch using random batches of data.\"\"\"\n","        for X, y_true in tqdm(\n","            self.train_dataset.iterate(self.batch_size),\n","            \"Training steps\",\n","            total=len(self.train_dataset) // self.batch_size + 1,\n","            leave=False,\n","        ):\n","            y_pred = self.model(X)\n","            grad = self.model.gradient(X, y_pred, y_true)\n","            self.model.update(grad, self.learning_rate)\n","\n","    def eval(self, dataset: Dataset) -> dict[str, float]:\n","        \"\"\"Evaluate the given dataset.\"\"\"\n","        metrics = {\n","            \"loss\": [],\n","            \"accuracy\": [],\n","        }\n","        for X, y_true in tqdm(\n","            dataset.iterate(self.batch_size),\n","            \"Evaluation steps\",\n","            total=len(dataset) // self.batch_size + 1,\n","            leave=False,\n","        ):\n","            y_pred = self.model(X)\n","            metrics[\"loss\"].append(self.loss(y_pred, y_true))\n","            metrics[\"accuracy\"].append(self.accuracy(y_pred, y_true))\n","\n","        metrics[\"loss\"] = np.mean(metrics[\"loss\"])\n","        metrics[\"accuracy\"] = np.mean(metrics[\"accuracy\"])\n","        return metrics\n","\n","    def start(self) -> dict[str, list[float]]:\n","        \"\"\"Launch the training.\n","\n","        ---\n","        Returns:\n","            The metrics of the test set at each epoch.\n","        \"\"\"\n","        metrics = {\n","            \"loss\": [],\n","            \"accuracy\": [],\n","            \"epoch\": [],\n","        }\n","\n","        for e in tqdm(range(1, self.n_epochs + 1), \"Epoch\"):\n","            self.train()\n","\n","            m = self.eval(self.test_dataset)\n","            metrics[\"loss\"].append(m[\"loss\"])\n","            metrics[\"accuracy\"].append(m[\"accuracy\"])\n","            metrics[\"epoch\"].append(e)\n","\n","        return metrics\n","\n","    @staticmethod\n","    def accuracy(y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n","        \"\"\"Compute the accuracy.\n","\n","        ---\n","        Args:\n","            y_pred: Model's predictions.\n","                Shape of [batch_size, n_classes].\n","            y_true: Ground truth.\n","                Shape of [batch_size, n_classes].\n","\n","        ---\n","        Returns:\n","            The mean accuracy.\n","        \"\"\"\n","        pass\n","\n","    @staticmethod\n","    def loss(y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n","        \"\"\"Compute the cross-entropy loss.\n","\n","        ---\n","        Args:\n","            y_pred: Model's predictions.\n","                Shape of [batch_size, n_classes].\n","            y_true: Ground truth.\n","                Shape of [batch_size, n_classes].\n","\n","        ---\n","        Returns:\n","            The mean cross-entropy loss.\n","        \"\"\"\n","        pass"],"metadata":{"id":"tXr-vXT7UPvq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Régression logistique\n","\n","La régression logistique peut être vue simplement comme un réseau de neurones à une seule couche. Le calcul du gradient ne nécessite pas de backpropagation.\n","\n","Implémentez le forward, l'update et le calcul du gradient du modèle. Pour le calcul du gradient, on vous donne en input tout ce dont vous avez besoin.\n","\n","Commencez par implémenter le modèle sans utiliser de biais dans la couche linéaire."],"metadata":{"id":"MZkpmUG2vaBj"}},{"cell_type":"code","source":["import math\n","\n","from scipy.special import softmax\n","\n","\n","class LogisticModel:\n","    weights: np.ndarray\n","\n","    def __init__(self, input_dim: int, n_classes: int, seed: int = 0):\n","        rng = np.random.default_rng(seed)\n","        lim = 1 / math.sqrt(input_dim)\n","        self.weights = rng.uniform(-lim, lim, (n_classes, input_dim))\n","\n","    def __call__(self, X: np.ndarray) -> np.ndarray:\n","        \"\"\"Do the forward pass.\n","\n","        ---\n","        Args:\n","            X: A batch of images.\n","                Shape of [batch_size, input_dim].\n","\n","        ---\n","        Returns:\n","            The predictions.\n","                Shape of [batch_size, n_classes].\n","        \"\"\"\n","        pass\n","\n","    def update(self, grad: np.ndarray, lr: float):\n","        \"\"\"Update the weights based on the given gradient and learning rate.\n","\n","        ---\n","        Args:\n","            grad: The gradient.\n","                Shape of `self.weights`.\n","            lr: The learning rate of the update.\n","        \"\"\"\n","        pass\n","\n","    @staticmethod\n","    def gradient(X: np.ndarray, y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n","        \"\"\"Compute the gradient.\n","\n","        ---\n","        Args:\n","            X: Batch of images.\n","                Shape of [batch_size, input_dim].\n","            y_pred: Model's predictions.\n","                Shape of [batch_size, n_classes].\n","            y_true: Ground truth.\n","                Shape of [batch_size, n_classes].\n","\n","        ---\n","        Returns:\n","            The gradient.\n","                Shape of `self.weights`.\n","        \"\"\"\n","        pass"],"metadata":{"id":"pLpaMq7AUN0f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Entraînement du modèle\n","\n","Essayez plusieurs hyper-paramètres et observez l'impacte qu'ils sont sur l'entraînement du modèle."],"metadata":{"id":"-KV1L2y7vcph"}},{"cell_type":"code","source":["x, y = train_dataset[0]\n","input_dim, n_classes = len(x), len(y)\n","model = LogisticModel(input_dim, n_classes)\n","lr = ...\n","epochs = ...\n","batch_size = ...\n","\n","trainer = Trainer(model, train_dataset, test_dataset, lr, batch_size, epochs)\n","metrics = trainer.start()\n","\n","fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n","axes[0].plot(metrics[\"epoch\"], metrics[\"loss\"])\n","axes[1].plot(metrics[\"epoch\"], metrics[\"accuracy\"])\n","\n","axes[0].set_title(\"Loss\")\n","axes[1].set_title(\"Accuracy\")\n","\n","axes[0].set_xlabel(\"Epoch\")\n","axes[1].set_xlabel(\"Epoch\")\n","\n","fig.tight_layout()\n","plt.show(fig)"],"metadata":{"id":"o01I-KGVlCTn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x, y_true = next(iter(train_dataset.iterate(16)))\n","y_pred = model(x)\n","fig = dataset.show_images(x, y_pred, n_cols=4)\n","fig.suptitle(\"model's predictions\")\n","fig.tight_layout()\n","plt.show(fig)"],"metadata":{"id":"JCo02fhFl3zH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Neural Network\n","\n","Il faut maintenant passer au niveau supérieur en implémentant votre propre réseau de neurones. Pour cela vous aurez besoin d'implémenter la backpropagation.\n","\n","Le modèle que vous implémentez est un simple MLP, avec un nombre variable de couches.\n","\n","Implémentez le forward, l'update et le calcul du gradient du modèle.\n","Le calcul du gradient doit se faire en utiliser les équations de la backpropagation.\n","\n","Commencez par implémenter le modèle sans utiliser de biais dans les couches linéaires.\n","\n","**Note:**\n","\n","**Vous devez sauvegarder les activations intermédiaires lors du forward afin de les réutiliser lors du backward.**"],"metadata":{"id":"oqSu4AxQXROs"}},{"cell_type":"code","source":["import math\n","\n","\n","class MLP:\n","    weights: list[np.ndarray]\n","    H: list[np.ndarray]  # Activations intermédiaires.\n","\n","    def __init__(self, input_dim: int, n_classes: int, hidden_dim: int, n_layers: int, seed: int = 0):\n","        rng = np.random.default_rng(seed)\n","\n","        # Initialize the weights here.\n","        ...\n","\n","\n","    def __call__(self, X: np.ndarray) -> np.ndarray:\n","        \"\"\"Do the forward pass. Save the intermediate computations for the\n","        backpropagation algorithm.\n","\n","        ---\n","        Args:\n","            X: A batch of images.\n","                Shape of [batch_size, input_dim].\n","\n","        ---\n","        Returns:\n","            The predictions.\n","                Shape of [batch_size, n_classes].\n","        \"\"\"\n","        pass\n","\n","    def gradient(self, _, y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n","        \"\"\"Compute the gradient using the backpropagation rules.\n","\n","        Remember to use the saved activations of the last forward pass.\n","\n","        ---\n","        Args:\n","            _: Batch of images (ignored here).\n","            y_pred: Model's predictions.\n","                Shape of [batch_size, n_classes].\n","            y_true: Ground truth.\n","                Shape of [batch_size, n_classes].\n","\n","        ---\n","        Returns:\n","            The gradient.\n","                Shape of `self.weights`.\n","        \"\"\"\n","        pass\n","\n","    def update(self, grads: list[np.ndarray], lr: float):\n","        \"\"\"Update the weights based on the given gradient and learning rate.\n","\n","        ---\n","        Args:\n","            grad: The gradient.\n","                Shape of `self.weights`.\n","            lr: The learning rate of the update.\n","        \"\"\"\n","        pass"],"metadata":{"id":"d9BmABN8s52C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Entraînement du modèle\n","\n","Pareil, vous pouvez tester plusieurs hyperparamètres. Ce modèle est encore plus sensible aux choix de vos hyperparamètres!\n","\n","Commencez par quelque chose de simple (une seule couche cachée et lr faible).\n","Un bon modèle devrait être au moins aussi bon que votre régression logistique (mais il peut prendre plus de temps à s'entraîner !)."],"metadata":{"id":"xyNpRlYhiSr9"}},{"cell_type":"code","source":["x, y = train_dataset[0]\n","input_dim, n_classes = len(x), len(y)\n","lr = ...\n","epochs = ...\n","batch_size = ...\n","hidden_dim = ...\n","n_hidden_layers = ...\n","\n","model = MLP(input_dim, n_classes, hidden_dim, n_hidden_layers)\n","\n","trainer = Trainer(model, train_dataset, test_dataset, lr, batch_size, epochs)\n","metrics = trainer.start()\n","\n","fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n","axes[0].plot(metrics[\"epoch\"], metrics[\"loss\"])\n","axes[1].plot(metrics[\"epoch\"], metrics[\"accuracy\"])\n","\n","axes[0].set_title(\"Loss\")\n","axes[1].set_title(\"Accuracy\")\n","\n","axes[0].set_xlabel(\"Epoch\")\n","axes[1].set_xlabel(\"Epoch\")\n","\n","fig.tight_layout()\n","plt.show(fig)"],"metadata":{"id":"gkxiNliIXREi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x, y_true = next(iter(train_dataset.iterate(16)))\n","y_pred = model(x)\n","fig = train_dataset.show_images(x, y_pred, n_cols=4)\n","fig.suptitle(\"model's predictions\")\n","fig.tight_layout()\n","plt.show(fig)"],"metadata":{"id":"ttsU3l0qc1fr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DViRL9K6T21Q"},"execution_count":null,"outputs":[]}]}